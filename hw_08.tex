    \documentclass[11pt]{article}

% ------------------------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% ------------------------------------------------------------------------------

\usepackage[margin=.8in,top=1.1in,bottom=1.1in]{geometry} % page layout
\usepackage{amsmath,amsthm,amssymb,amsfonts} % math things
\usepackage{graphicx} % include graphics
\usepackage{fancyhdr} % header customization
\usepackage{titlesec} % help with section naming
\usepackage{listings}% display source code
\usepackage{color}

%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},   commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}

%"mystyle" code listing set
\lstset{style=mystyle}

% naming sections
\titleformat{\section}{\bf}{Problem \thesection}{0.5em}{}
\newcommand{\exercise}{\section{}}

% headers
\pagestyle{fancy}
\fancyhf{} % clear all
\fancyhead[L]{\sffamily\small Machine Learning I --- Homework}
\fancyhead[R]{\sffamily\small Page \thepage}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.2pt}
\markright{\hrulefill\quad}

\newcommand{\hwhead}[4]{
\begin{center}
\sffamily\large\bfseries Machine Learning I
\vspace{2mm}
\normalfont

#2\\
#3\\
\texttt{#4}
\end{center}
\vspace{6mm} \hrule \vspace{4mm}
}

%%
\DeclareMathOperator{\E}{\mathbb{E}}
\newcommand{\intf}[4]{\int_{#1}^{#2} \! #3 \, \mathrm{d}#4}
\newcommand{\sumf}[3]{\sum_{#1}^{#2} #3}
\newcommand{\tf}[1]{{#1}^{\intercal}}
\newcommand{\ifelse}[4]{%
\begingroup
\begin{cases}
  {#1} \text{, if {#2}} \\
  {#3} \text{, if {#4}}
\end{cases}
\endgroup
}

\newcommand{\gradf}[2]{\nabla_{#1}{\partial #2}}

\begin{document}

\exercise

No, there is no such guarantee. There may be datapoints, which either (1) violate the margin, i.e. $0 \le \xi_i \le 1$ or (2) are on the wrong side of the hyperplane, i.e. $y_i (\tf{w} x_i + b) \ge 1 - \xi_i$ does not hold, in this case $\xi_i > 1$.

\exercise

$C < 0$ we have from the Lagrangian of slack variables that $0 \le \alpha_i \le C$ but this obviously does not hold. If $ C = 0$, then we have no penalty term in the optimization problem and from the Lagrangian of slack variables it follows that
\begin{align*}
\alpha_i &= - \mu_i \\
0 \le \alpha_i &= - \mu_i \\
0 &\le -\mu_i  \tag{violation of dual feasibility of Lagrangian multipliers $\mu_i \ge 0$ }
\end{align*}

\noindent Thus $C > 0$.

\exercise

We prove that $ K(x,y) = ( \tf{x} y + c)^d $ is a kernel by the construction rules.

\begin{itemize}
\item $K_1(x, y) = \tf{x} y$ is a kernel by Rule 5, where $B = I$.
\item $K_2(x, y) = c$ is a kernel by Rule 4, where $ \phi(z) = \sqrt{c}, \phi : \mathbb{R}^n \rightarrow \mathbb{R}^m, m = 1$ and $K_3(x, y) = \tf{x} y $
\item $K_3(x, y) = K_1(x, y) + K_2(x, y)$ is a kernel by Rule 1
\item $K_4(x, y) = K_3(x, y)^d$ is a kernel by recursive application of Rule 3
\end{itemize}

Thus $ K(x,y) = ( \tf{x} y + c)^d $ is a valid kernel.

\exercise

\exercise

\exercise

\exercise

\begin{align*}
|| x - x^{(s_i)} ||_2 &= \sqrt{\sumf{j = 1}{M}{(x_j - x_j^{(s_i)})^2}} \\
&= \sumf{j = 1}{M}{(x_j - x_j^{(s_i)})^2} \tag{squared distance does not change k nearest neighbors of $x$} \\
&= \sumf{j = 1}{M}{(\phi(x_j) - \phi(x_j^{(s_i)}))^2} \tag{ by feature map $\phi(x)$ } \\
&= \tf{(\phi(x) - \phi(x^{(s_i)}))} (\phi(x) - \phi(x^{(s_i)})) \\
&= \tf{\phi(x)} \phi(x) - 2 \tf{ \phi(x) }  \phi(x^{(s_i)}) + \tf{ \phi(x^{(s_i)}) }  \phi(x^{(s_i)}) \\
&= K(x, x)  + K(x^{(s_i)}, x^{(s_i)}) - 2 K(x, x^{(s_i)})
\end{align*}

\end{document}
