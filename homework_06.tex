    \documentclass[11pt]{article}

% ------------------------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% ------------------------------------------------------------------------------

\usepackage[margin=.8in,top=1.1in,bottom=1.1in]{geometry} % page layout
\usepackage{amsmath,amsthm,amssymb,amsfonts} % math things
\usepackage{graphicx} % include graphics
\usepackage{fancyhdr} % header customization
\usepackage{titlesec} % help with section naming
\usepackage{listings}% display source code
\usepackage{color}

%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},   commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}

%"mystyle" code listing set
\lstset{style=mystyle}

% naming sections
\titleformat{\section}{\bf}{Problem \thesection}{0.5em}{}
\newcommand{\exercise}{\section{}}

% headers
\pagestyle{fancy}
\fancyhf{} % clear all
\fancyhead[L]{\sffamily\small Machine Learning I --- Homework}
\fancyhead[R]{\sffamily\small Page \thepage}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.2pt}
\markright{\hrulefill\quad}

\newcommand{\hwhead}[4]{
\begin{center}
\sffamily\large\bfseries Machine Learning I Worksheet #2
\vspace{2mm}
\normalfont

#2\\
#3\\
\texttt{#4}
\end{center}
\vspace{6mm} \hrule \vspace{4mm}
}

%%
\DeclareMathOperator{\E}{\mathbb{E}}
\newcommand{\intf}[4]{\int_{#1}^{#2} \! #3 \, \mathrm{d}#4}
\newcommand{\sumf}[3]{\sum_{#1}^{#2} #3}
\newcommand{\tf}[1]{{#1}^{\intercal}}
\newcommand{\ifelse}[4]{%

\begingroup
\begin{cases}
  {#1} \text{, if {#2}} \\
  {#3} \text{, if {#4}}
\end{cases}
\endgroup
}

\newcommand{\gradf}[2]{\nabla_{#1}{#2}}

\begin{document}

\exercise

Proofs make use of the properties/rules from Lecture 06 - slides (7, 15, 16, 17).
\begin{itemize}
\item $f(x,y,z) = 3x + e^{y + z} - \text{min} \{  -x^2, \quad \text{log}(y) \}$, on $D = ( \text{-}100, 100) \times (1, 50) \times (10, 20) $ is convex.
\begin{itemize}
\item $3x$ is convex by Rule 1.
\item $e^{y + z}$ is convex by Rule 1 and 2.
\item $x^2$ is convex (i.e. its Hessian, a 3 $\times$ 3 matrix, which is zero everywhere except $\frac{\partial^2 x^2}{\partial^2 x} = 2$ is positive-semi-definite on $D$, thus making $-x^2$  concave. $\text{log}(y)$ is concave. $\text{min} \{  -x^2, \quad \text{log}(y) \}$ returns $-x^2$ by construction of $D$, making $- \text{min} \{  -x^2, \quad \text{log}(y) \}$ convex.
\end{itemize}
By Rule 1, the sum of these convex functions is also convex. It follows, $f(x,y,z)$ is convex.
\item $f(x,y) = y x^3 - 2 y x^2 + y + 4$, $D = (\text{-}10, 10) \times (\text{-}10, 10)$ is not convex because the Hessian of $x^3$, a 2 $\times$ 2 matrix, which is zero everywhere except $\frac{\partial^2 x^3}{\partial^2 x} = 6x$ is clearly not positive-semi-definite for all $z \in D$.
\item $f(x) = \text{log}(x) + x^3$ and $D = (1, \infty)$ is not convex because $\text{log}(x)$ is concave by definition.
\item $f(x) = \text{-min}(2 \text{log}(2x), \text{-}x^2 + 4x \quad \text{-} 32)$, $D = \mathbb{R}^+$ is convex, because $\text{-}x^2 + 4x \quad \text{-} 32$ is a concave quadratic function which reaches its global maximum of $28$ at $x = 2$ (e.g. use high-school math to determine maximum). On the other hand $ \text{log}(2x)$ is non-negative, meaning that $\text{min}(\cdot, \cdot)$ always returns the quadratic function. Finally, the negative sign makes $f(x)$ convex.
\end{itemize}

\exercise
Let $x, y \in \mathbb{R}^d, \alpha \in [0, 1]$.
\begin{align*}
h( \alpha x + (1 - \alpha) y ) &= f_1( \alpha x + (1 - \alpha) y ) + f_2( \alpha x + (1 - \alpha) y ) \tag{by def.} \\
&\le \alpha f_1(x) + (1 - \alpha) f_1(y) + \alpha f_2(x) + (1 - \alpha) f_2(y) \tag{by convexity of $f_1, f_2$}  \\
&= \alpha f_1(x) + \alpha f_2(x) + (1 - \alpha) f_1(y) + (1 - \alpha) f_2(y) \\
&= \alpha h(x) + (1 - \alpha) h(y)
\end{align*}

\exercise

Proof by counter example. Let $f_1(x) = x^2$ and $f_2(x) = x$. Both functions are convex (by Exercise 1). But, $g(x) = f_1(x) \cdot f_2(x) = x^3$ is not convex on $\mathbb{R}$ because its second order derivative $6x$ is not non-negative over this interval.
\exercise

Proof by contradiction. Assume $\gradf{\theta}{f(\theta^*)} = 0$ and $\theta^*$ is not a global minimum. By first order convexity of $f$ we have $ \forall x, y : f(y) \ge f(x) + \tf{(y - x)} \gradf{x}{f(x)}$. For $x = \theta^*$ we get
$f(y) \ge f(\theta^*)$. But $\theta^*$ is not global minimum by assumption thus we get a contradiction. Thus $\theta^*$ must be the global minimum.
\exercise
See notebook below.
\end{document}
