    \documentclass[11pt]{article}

% ------------------------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% ------------------------------------------------------------------------------

\usepackage[margin=.8in,top=1.1in,bottom=1.1in]{geometry} % page layout
\usepackage{amsmath,amsthm,amssymb,amsfonts} % math things
\usepackage{graphicx} % include graphics
\usepackage{fancyhdr} % header customization
\usepackage{titlesec} % help with section naming
\usepackage{listings}% display source code
\usepackage{color}

%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},   commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}

%"mystyle" code listing set
\lstset{style=mystyle}

% naming sections
\titleformat{\section}{\bf}{Problem \thesection}{0.5em}{}
\newcommand{\exercise}{\section{}}

% headers
\pagestyle{fancy}
\fancyhf{} % clear all
\fancyhead[L]{\sffamily\small Machine Learning I --- Homework}
\fancyhead[R]{\sffamily\small Page \thepage}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.2pt}
\markright{\hrulefill\quad}

\newcommand{\hwhead}[4]{
\begin{center}
\sffamily\large\bfseries Machine Learning I Worksheet #2
\vspace{2mm}
\normalfont

#2\\
#3\\
\texttt{#4}
\end{center}
\vspace{6mm} \hrule \vspace{4mm}
}

%%
\DeclareMathOperator{\E}{\mathbb{E}}
\newcommand{\intf}[4]{\int_{#1}^{#2} \! #3 \, \mathrm{d}#4}
\newcommand{\sumf}[3]{\sum_{#1}^{#2} #3}
\newcommand{\tf}[1]{{#1}^{\intercal}}
\newcommand{\ifelse}[4]{%
\begingroup
\begin{cases}
  {#1} \text{, if {#2}} \\
  {#3} \text{, {#4}}
\end{cases}
\endgroup
}

\newcommand{\gradf}[1]{\frac{\nabla}{{\partial #1}}}
\newcommand{\partialf}[1]{\frac{\partial}{{\partial #1}}}

\begin{document}

\exercise

\begin{align*}
\E[x] &= \intf{-\infty}{\infty}{x \cdot \sumf{k}{}{\pi_k \mathcal{N}(x | \mu_k, \Sigma_k)} }{x} \\
&= \sumf{k}{}{ \intf{-\infty}{\infty}{\pi_k \mathcal{N}(x | \mu_k, \Sigma_k)}{x}} \\
&= \sumf{k}{}{ \pi_k \intf{-\infty}{\infty}{ \mathcal{N}(x | \mu_k, \Sigma_k)}{x}} \\
&= \sumf{k}{}{ \pi_k \mu_k } = \mu \tag{1.1}
\end{align*}

\begin{align*}
\text{Cov}[x] &= \E[x \tf{x}] - \E[x] \tf{\E[x]} \\
&= \E[x \tf{x}] - \mu \tf{\mu} \tag{by 1.1} \\
&= \mu \tf{\mu} + \sumf{k}{}{\Sigma_k} - \mu \tf{\mu} \tag{by Bishop 2.62} \\
&= \sumf{k}{}{\Sigma_k} = \Sigma \\
\end{align*}

\exercise

Given $\Sigma_k = \sigma^2 I, \sigma^2 \rightarrow 0$, we have for the exponential in $\mathcal{N}( \cdot, \cdot )$

\begin{align*}
e^{-\frac{1}{2} \tf{ (x_i - \mu_k) } (\sigma^2 I)^{-1} (x_i - \mu_k)} &= e^{-\frac{1}{2 \sigma^2} \tf{ (x_i - \mu_k) } (x_i - \mu_k)} \rightarrow 0 \tag{ $\sigma^2 \rightarrow 0$}
\end{align*}

\noindent \textbf{E-Step}

\begin{align*}
\gamma(z_{ik}) = \frac{ \pi_k \mathcal{N}( x_i | \mu_k, \Sigma_k ) }{ \pi_j \mathcal{N}( x_i | \mu_j, \Sigma_j ) }
\end{align*}

\noindent It's important to notice that if a datapoint lies far away from a centroid, i.e. $x_i - \mu_k >> 0$ then the exponential term associated to this Gaussian goes much faster to zero than the other ones. Thus for a datapoint $x_i$ we have

\begin{align*}
\gamma(z_{ik})  &= \ifelse{1}{$ \text{argmin}_k \quad || x_i - \mu_k ||_2 $}{0}{\text{else}} \tag{2.1}
\end{align*}

\noindent \textbf{M-Step}

\noindent Thus the estimates $\mu_k^{\text{new}}, \Sigma_k^{\text{new}}, \pi_k^{\text{new}} $ (cf. slide 29, lecture) become

\begin{align*}
\mu_k^{\text{new}} &= \frac{1}{N_k} \sumf{i}{N}{z_{ik} x_i} \\
\Sigma_k^{\text{new}} &= \sigma^2 I \tag{by 2.1 this parameter stays the same as initialized in the beginning} \\
\pi_k^{\text{new}} &= \frac{N_k}{N}, N_k = \sumf{i}{N}{z_{ik}}
\end{align*}

\noindent Thus we see in the case $\Sigma_k = \sigma^2 I, \sigma^2 \rightarrow 0$, the EM-Algorithm is equivalent to Lloyd's algorithm.

\exercise

See below.

\end{document}
