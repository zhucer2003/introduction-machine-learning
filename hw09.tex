\documentclass[11pt]{article}

% ------------------------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% ------------------------------------------------------------------------------

\usepackage[margin=.8in,top=1.1in,bottom=1.1in]{geometry} % page layout
\usepackage{amsmath,amsthm,amssymb,amsfonts} % math things
\usepackage{graphicx} % include graphics
\usepackage{fancyhdr} % header customization
\usepackage{titlesec} % help with section naming
\usepackage{listings}% display source code
\usepackage{color}

%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},   commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}

%"mystyle" code listing set
\lstset{style=mystyle}

% naming sections
\titleformat{\section}{\bf}{Problem \thesection}{0.5em}{}
\newcommand{\exercise}{\section{}}

% headers
\pagestyle{fancy}
\fancyhf{} % clear all
\fancyhead[L]{\sffamily\small Machine Learning I --- Homework}
\fancyhead[R]{\sffamily\small Page \thepage}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.2pt}
\markright{\hrulefill\quad}

\newcommand{\hwhead}[4]{
\begin{center}
\sffamily\large\bfseries Machine Learning I Worksheet #2
\vspace{2mm}
\normalfont

#2\\
#3\\
\texttt{#4}
\end{center}
\vspace{6mm} \hrule \vspace{4mm}
}

%%
\DeclareMathOperator{\E}{\mathbb{E}}
\newcommand{\intf}[4]{\int_{#1}^{#2} \! #3 \, \mathrm{d}#4}
\newcommand{\sumf}[3]{\sum_{#1}^{#2} #3}
\newcommand{\tf}[1]{{#1}^{\intercal}}
\newcommand{\ifelse}[4]{%
\begingroup
\begin{cases}
  {#1} \text{, if {#2}} \\
  {#3} \text{, if {#4}}
\end{cases}
\endgroup
}

\newcommand{\gradf}[2]{\frac{\nabla_{#1}}{{\partial #2}}}
\newcommand{\partialf}[1]{\frac{\partial}{{\partial #1}}}

\begin{document}

\exercise



\exercise

\exercise

\begin{align*}
\partialf{x}(\text{tanh}(x)) &= \partialf{x}(\frac{e^x - e^{-x}}{e^x + e^{-x}}) \\
&= \frac{ ( e^x + e^{-x} )( e^x + e^{-x}) - ( e^x - e^{-x} )( e^x - e^{-x} )}{ ( e^x + e^{-x} )^2 } \\
&= 1 - \text{tanh}^2(x)
\end{align*}

It substantially simplifies the calculations during backpropagation when computing the local gradient at an intermediate node and when passing the gradient to the lower layers.

\exercise
Some useful facts
$$\sigma(x) = \frac{1}{1 + e^{-x}} = \frac{e^x}{1 + e^{x}} $$
$$\sigma(-x) = 1 - \sigma(x)$$
$$Ber(y | x ) \sim x^{\frac{1 + y}{2}} (1 - x)^{\frac{1 - y}{2}} \quad \text{for} \quad y = \{-1, 1 \} $$


\noindent Transform network output from $0 \leq f(x_i, W) \leq 1$ to $-1 \leq f(x_i, W) \leq 1$
\begin{align*}
0 \leq f(x_i, W) \leq 1 &\Leftrightarrow \\
0 \leq 2f(x_i, W) \leq 2 &\Leftrightarrow  \\
-1 \leq 2f(x_i, W) - 1 \leq 1 &\Leftrightarrow 
\end{align*}

\noindent \textbf{Goal:} Modified binary cross-entropy loss should be: $E(W) = - \sumf{i=1}{N}{(\text{ln}(1 + e^{(-y_i f(x_i, W))}))}$.


\exercise

\exercise

We would stop training approximately at iteration 50, because while the training error keeps decreasing we have an increase in the validation error, which is a sign of overfitting. After we take the weights of the network at this iteration we test it one final time on the test set. Note that we do not get the optimal test error as we would if we would continue with the training until iteration 80.

\exercise

\exercise

\begin{align*}
\frac{e^{x_i}}{\sumf{i=1}{N}{e^{x_i}}} &= \frac{e^{a}}{e^{a}} \frac{e^{x_i}}{\sumf{i=1}{N}{e^{x_i}}} \\
&= \frac{e^{x_i - a}}{\sumf{i=1}{N}{e^{x_i - a}}}
\end{align*}

\exercise

The binary cross-entropy is equivalent to

\begin{align*}
-[y \text{log}(\sigma(x)) + (1 - y) \text{log}(1 - \sigma(x)) ] &= -[y \text{log}(\frac{e^x}{1 + e^x}) + (1 - y) \text{log}(1 - \frac{e^x}{1 + e^x}) ] \\
&= -[ y \text{log}e^x -y \text{log}(1 + e^x) + (1 - y)( -\text{log}(1 + e^x ))] \\
&= -[yx - \text{log}(1 + e^x )] \\
&= -yx + \text{log}(1 + e^x ) \\
&= -yx + \text{log}(e^x(e^{-x} + 1 )) \\
&= x -yx + \text{log}( 1 + e^{-x} ) \\
&= \text{max}(x,0) -yx + \text{log}( 1 + e^{-|x|} ) \tag{see below}
\end{align*}

\noindent The binary cross-entropy and in general any well-defined loss function should satisfy the following properties : For correctly classified datapoints we want to have zero loss and for incorrectly classified points, we want to have a large loss. This can be easily verified for the binary cross-entropy, e.g. if x is a large positive score and y = 0, i.e. x is missclassified then we get a large loss value, contrary if y = 1 we get zero loss, as expected. Our stable binary cross-entropy should also satisfy this properties.
\\
\noindent For $x \rightarrow -\infty$ we get an overflow  when implementing it on computers. To overcome this issue, we can map any negative x to a positive value using the $\text{abs}(\cdot)$ function. To ensure that for missclassified datapoints we penalize with a non-zero loss we have to add the term $\text{max}(x, 0)$ to the loss function (Note that in the casse of correctly missclassified datapoints this term either is zero or cancels out with $yx$, which ensures zero loss).

\end{document}
