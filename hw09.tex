\documentclass[11pt]{article}

% ------------------------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% ------------------------------------------------------------------------------

\usepackage[margin=.8in,top=1.1in,bottom=1.1in]{geometry} % page layout
\usepackage{amsmath,amsthm,amssymb,amsfonts} % math things
\usepackage{graphicx} % include graphics
\usepackage{fancyhdr} % header customization
\usepackage{titlesec} % help with section naming
\usepackage{listings}% display source code
\usepackage{color}

%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},   commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}

%"mystyle" code listing set
\lstset{style=mystyle}

% naming sections
\titleformat{\section}{\bf}{Problem \thesection}{0.5em}{}
\newcommand{\exercise}{\section{}}

% headers
\pagestyle{fancy}
\fancyhf{} % clear all
\fancyhead[L]{\sffamily\small Machine Learning I --- Homework}
\fancyhead[R]{\sffamily\small Page \thepage}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.2pt}
\markright{\hrulefill\quad}

\newcommand{\hwhead}[4]{
\begin{center}
\sffamily\large\bfseries Machine Learning I Worksheet #2
\vspace{2mm}
\normalfont

#2\\
#3\\
\texttt{#4}
\end{center}
\vspace{6mm} \hrule \vspace{4mm}
}

%%
\DeclareMathOperator{\E}{\mathbb{E}}
\newcommand{\intf}[4]{\int_{#1}^{#2} \! #3 \, \mathrm{d}#4}
\newcommand{\sumf}[3]{\sum_{#1}^{#2} #3}
\newcommand{\tf}[1]{{#1}^{\intercal}}
\newcommand{\ifelse}[4]{%
\begingroup
\begin{cases}
  {#1} \text{, if {#2}} \\
  {#3} \text{, if {#4}}
\end{cases}
\endgroup
}

\newcommand{\gradf}[2]{\frac{\nabla_{#1}}{{\partial #2}}}
\newcommand{\partialf}[1]{\frac{\partial}{{\partial #1}}}

\begin{document}

\exercise

\exercise

\exercise

\begin{align*}
\partialf{x}(\text{tanh}(x)) &= \partialf{x}(\frac{e^x - e^{-x}}{e^x + e^{-x}}) \\
&= \frac{ ( e^x + e^{-x} )( e^x + e^{-x}) - ( e^x - e^{-x} )( e^x - e^{-x} )}{ ( e^x + e^{-x} )^2 } \\
&= 1 - \text{tanh}^2(x)
\end{align*}

It substantially simplifies the calculations during backpropagation when computing the local gradient at an intermediate node and when passing the gradient to the lower layers.

\exercise

\exercise

\exercise

We would stop training approximately at iteration 50, because while the training error keeps decreasing we have an increase in the validation error, which is a sign of overfitting. After we take the weights of the network at this iteration we test it one final time on the test set. Note that we do not get the optimal test error as we would if we would continue with the training until iteration 80.

\exercise

\exercise

\begin{align*}
\frac{e^{x_i}}{\sumf{i=1}{N}{e^{x_i}}} &= \frac{e^{a}}{e^{a}} \frac{e^{x_i}}{\sumf{i=1}{N}{e^{x_i}}} \\
&= \frac{e^{x_i - a}}{\sumf{i=1}{N}{e^{x_i - a}}}
\end{align*}

\exercise

\end{document}
