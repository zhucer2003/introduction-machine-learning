\documentclass[11pt]{article}

% ------------------------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% ------------------------------------------------------------------------------

\usepackage[margin=.8in,top=1.1in,bottom=1.1in]{geometry} % page layout
\usepackage{amsmath,amsthm,amssymb,amsfonts} % math things
\usepackage{graphicx} % include graphics
\usepackage{fancyhdr} % header customization
\usepackage{titlesec} % help with section naming
\usepackage{listings}% display source code
\usepackage{color}

%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},   commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}

%"mystyle" code listing set
\lstset{style=mystyle}

% naming sections
\titleformat{\section}{\bf}{Problem \thesection}{0.5em}{}
\newcommand{\exercise}{\section{}}

% headers
\pagestyle{fancy}
\fancyhf{} % clear all
\fancyhead[L]{\sffamily\small Machine Learning I --- Homework}
\fancyhead[R]{\sffamily\small Page \thepage}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.2pt}
\markright{\hrulefill\quad}

\newcommand{\hwhead}[4]{
\begin{center}
\sffamily\large\bfseries Machine Learning I Worksheet #2
\vspace{2mm}
\normalfont

#2\\
#3\\
\texttt{#4}
\end{center}
\vspace{6mm} \hrule \vspace{4mm}
}

%%
\DeclareMathOperator{\E}{\mathbb{E}}
\newcommand{\intf}[4]{\int_{#1}^{#2} \! #3 \, \mathrm{d}#4}
\newcommand{\sumf}[3]{\sum_{#1}^{#2} #3}
\newcommand{\tf}[1]{{#1}^{\intercal}}
\newcommand{\ifelse}[4]{%
\begingroup
\begin{cases}
  {#1} \text{, if {#2}} \\
  {#3} \text{, if {#4}}
\end{cases}
\endgroup
}

\newcommand{\gradf}[2]{\frac{\nabla_{#1}}{{\partial #2}}}
\newcommand{\partialf}[1]{\frac{\partial}{{\partial #1}}}

\begin{document}

\exercise

Basis functions can transform an input from a not linearly separable space to a linearly separable space. A neural network performs many such transformations, where the parameters of these basis functions are learned rather than hand-crafted. Using many basis functions, where each of them disentangles the dataspace even better, a neural network can separate datapoints which could be not linearly separated in the original input space.

\exercise

\exercise

\begin{align*}
\partialf{x}(\text{tanh}(x)) &= \partialf{x}(\frac{e^x - e^{-x}}{e^x + e^{-x}}) \\
&= \frac{ ( e^x + e^{-x} )( e^x + e^{-x}) - ( e^x - e^{-x} )( e^x - e^{-x} )}{ ( e^x + e^{-x} )^2 } \\
&= 1 - \text{tanh}^2(x)
\end{align*}

It substantially simplifies the calculations during backpropagation when computing the local gradient at an intermediate node and when passing the gradient to the lower layers.

\exercise

For deriving the new loss function under the specified requirements, we follow a similar approach as to the derivation in the case of using sigmoid (cf. Lecture Linear Classification, slide 28). We use $ \text{tanh}( \cdot ) $ instead of $\sigma( \cdot )$. However to be able to still interpret the output as probabilities and define it as a Bernoulli experiment we need to rescale it to be in $[0, 1]$.

\begin{align*}
p( y = 1 | x) &=\frac{\text{tanh}(x) + 1}{2}, \quad y = 1 \\
p( y = -1 | x) &=1 - \frac{\text{tanh}(x) + 1}{2}, \quad y = -1
\end{align*}
or more compactly (and useful later) as:
$x^{\frac{1 + y}{2}} \cdot (1 - x)^{\frac{1 - y}{2}}$

\begin{align*}
E(W) &= -\sumf{i = 1}{N}{ \frac{1 + y}{2} \text{log}(\frac{\text{tanh}(x_i, W) + 1}{2}) + \frac{1 - y}{2} \text{log}(1 - \frac{\text{tanh}(x_i, W) + 1}{2})} \\
&= -\sumf{i = 1}{N}{ \frac{1 + y}{2} \text{log}(\frac{f(x_i, W) + 1}{2}) + \frac{1 - y}{2} \text{log}(1 - \frac{f(x_i, W) + 1}{2})}
\end{align*}

Clearly the loss is zero for correctly classified datapoints and large for missclassified datapoints. Furthermore an output of $\text{tanh}(x), x \rightarrow -1$ and  $\text{tanh}(x), x \rightarrow 1$ is transformed to a low (probability) value respectively high value. By construction the activation function to be chosen is $\text{tanh}(\cdot)$

\exercise

\exercise

We would stop training approximately at iteration 50, because while the training error keeps decreasing we have an increase in the validation error, which is a sign of overfitting. After we take the weights of the network at this iteration we test it one final time on the test set. Note that we do not get the optimal test error as we would if we would continue with the training until iteration 80.

\exercise

\begin{align*}
y &= \text{log}(\sumf{i=1}{N}{ e^{x_i} }) \\
&= \text{log}(\sumf{i=1}{N}{ \frac{e^a}{e^a} e^{x_i} }) \\
&= \text{log}(e^a \sumf{i=1}{N}{e^{-a} e^{x_i} }) \\
&= \text{log}(e^a) + \text{log}(\sumf{i=1}{N}{e^{-a} e^{x_i} }) \\
&= a + \text{log}(\sumf{i=1}{N}{e^{x_i - a} })
\end{align*}

\exercise

\begin{align*}
\frac{e^{x_i}}{\sumf{i=1}{N}{e^{x_i}}} &= \frac{e^{a}}{e^{a}} \frac{e^{x_i}}{\sumf{i=1}{N}{e^{x_i}}} \\
&= \frac{e^{x_i - a}}{\sumf{i=1}{N}{e^{x_i - a}}}
\end{align*}

\exercise

The binary cross-entropy is equivalent to

\begin{align*}
-[y \text{log}(\sigma(x)) + (1 - y) \text{log}(1 - \sigma(x)) ] &= -[y \text{log}(\frac{e^x}{1 + e^x}) + (1 - y) \text{log}(1 - \frac{e^x}{1 + e^x}) ] \\
&= -[ y \text{log}e^x -y \text{log}(1 + e^x) + (1 - y)( -\text{log}(1 + e^x ))] \\
&= -[yx - \text{log}(1 + e^x )] \\
&= -yx + \text{log}(1 + e^x ) \\
&= -yx + \text{log}(e^x(e^{-x} + 1 )) \\
&= x -yx + \text{log}( 1 + e^{-x} ) \\
&= \text{max}(x,0) -yx + \text{log}( 1 + e^{-|x|} ) \tag{see below}
\end{align*}

\noindent The binary cross-entropy and in general any well-defined loss function should satisfy the following properties : For correctly classified datapoints we want to have zero loss and for incorrectly classified points, we want to have a large loss. This can be easily verified for the binary cross-entropy, e.g. if x is a large positive score and y = 0, i.e. x is missclassified then we get a large loss value, contrary if y = 1 we get zero loss, as expected. Our stable binary cross-entropy should also satisfy this properties.
\\
\noindent For $x \rightarrow -\infty$ we get an overflow  when implementing it on computers. To overcome this issue, we can map any negative x to a positive value using the $\text{abs}(\cdot)$ function. To ensure that for missclassified datapoints we penalize with a non-zero loss we have to add the term $\text{max}(x, 0)$ to the loss function (Note that in the casse of correctly missclassified datapoints this term either is zero or cancels out with $yx$, which ensures zero loss).

\end{document}
