\documentclass[11pt]{article}

% ------------------------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% ------------------------------------------------------------------------------

\usepackage[margin=.8in,top=1.1in,bottom=1.1in]{geometry} % page layout
\usepackage{amsmath,amsthm,amssymb,amsfonts} % math things
\usepackage{graphicx} % include graphics
\usepackage{fancyhdr} % header customization
\usepackage{titlesec} % help with section naming
\usepackage{listings}% display source code
\usepackage{color}

%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},   commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}

%"mystyle" code listing set
\lstset{style=mystyle}

% naming sections
\titleformat{\section}{\bf}{Problem \thesection}{0.5em}{}
\newcommand{\exercise}{\section{}}

% headers
\pagestyle{fancy}
\fancyhf{} % clear all
\fancyhead[L]{\includegraphics[scale=0.02]{easteregg} \sffamily\small Machine Learning I --- Homework}
\fancyhead[R]{\sffamily\small Page \thepage}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.2pt}
\markright{\hrulefill\quad}

\newcommand{\hwhead}[4]{
\begin{center}
\sffamily\large\bfseries Machine Learning I Worksheet #2
\vspace{2mm}
\normalfont

#2\\
#3\\
\texttt{#4}
\end{center}
\vspace{6mm} \hrule \vspace{4mm}
}

%%
\DeclareMathOperator{\E}{\mathbb{E}}
\newcommand{\intf}[4]{\int_{#1}^{#2} \! #3 \, \mathrm{d}#4}
\newcommand{\sumf}[3]{\sum_{#1}^{#2} #3}
\newcommand{\tf}[1]{{#1}^{\intercal}}
\newcommand{\ifelse}[4]{%
\begingroup
\begin{cases}
  {#1} \text{, if {#2}} \\
  {#3} \text{, otherwise {#4}}
\end{cases}
\endgroup
}

\newcommand{\gradf}[2]{\frac{\nabla_{#1}}{{\partial #2}}}
\newcommand{\partialf}[1]{\frac{\partial}{{\partial #1}}}

\begin{document}

\exercise

Basis functions can transform a not linearly separable input space to a linearly separable space. A neural network uses a hierarchical feature representation of the input space and these basis functions to linearly separate the input datapoints. The basis functions are adaptive in the sense that their coefficients are not hand-crafted but determined by learning.

\exercise

\begin{align*}
\text{tanh}(x) &= \frac{e^x - e^{-x}}{e^x + e^{-x}} \\
&= \frac{e^x}{e^x + e^{-x}} - \frac{e^{-x}}{e^x + e^{-x}} \\
&= \frac{1}{1 + e^{-2x}} - \frac{1}{1 + e^{2x}} \\
&= \sigma(2x) - \sigma(-2x) \\
&= \sigma(2x) - 1 + \sigma(2x) \\
&= 2\sigma(2x) - 1 \tag{2.1}
\end{align*}

\noindent W.l.o.g. assume one hidden layer neural network function $f(x, W)$ with $\text{tanh}(\cdot)$ activation function and identity function for output layer.
Furthermore $X \in \mathbb{R}^{D+1}, W_1 \in \mathbb{R}^{1 \times {(M+1)}}, W_0 \in \mathbb{R}^{(D+1) \times (M+1)}$.

\begin{align*}
f(x, W) &= \tf{W_1} \text{tanh}(\tf{W_0} X) \\
&= \tf{2W_1} \sigma(\tf{2W_0} X) - 1 \tag{by 2.1} \\
&= \tf{W_1^\prime} \sigma(\tf{W_0^\prime} X) - 1 \tag{$\tf{W_1^\prime} = 2W_1, \tf{W_0^\prime} = 2W_0$} \\
&= \tf{W_1^\prime} \sigma(\tf{W_0^\prime} X) \tag{by absorbing the constant "-1" in $W_1^\prime$ similar to absorbing the bias in general} \\
\end{align*}

\exercise

\begin{align*}
\partialf{x}(\text{tanh}(x)) &= \partialf{x}(\frac{e^x - e^{-x}}{e^x + e^{-x}}) \\
&= \frac{ ( e^x + e^{-x} )( e^x + e^{-x}) - ( e^x - e^{-x} )( e^x - e^{-x} )}{ ( e^x + e^{-x} )^2 } \\
&= 1 - \text{tanh}^2(x)
\end{align*}

\noindent It substantially simplifies the calculations during backpropagation when computing the local gradient at an intermediate node and when passing the gradient to the lower layers.

\exercise

For deriving the new loss function, we follow a similar approach as for the derivation in the case of using sigmoid (cf. Lecture Linear Classification, slide 28). We use $ \text{tanh}( \cdot ) $ instead of $\sigma( \cdot )$. However to be able to still interpret the output as probabilities and define it as a Bernoulli experiment we need to rescale it to be in $[0, 1]$.

\begin{align*}
p( y = 1 | x) &=\frac{\text{tanh}(x) + 1}{2}, \quad y = 1 \\
p( y = -1 | x) &=1 - \frac{\text{tanh}(x) + 1}{2}, \quad y = -1
\end{align*}
or more compactly as:
$x^{\frac{1 + y}{2}} \cdot (1 - x)^{\frac{1 - y}{2}}$

\begin{align*}
E(W) &= -\sumf{i = 1}{N}{ \frac{1 + y}{2} \text{log}(\frac{\text{tanh}(x_i, W) + 1}{2}) + \frac{1 - y}{2} \text{log}(1 - \frac{\text{tanh}(x_i, W) + 1}{2})} \\
&= -\sumf{i = 1}{N}{ \frac{1 + y}{2} \text{log}(\frac{f(x_i, W) + 1}{2}) + \frac{1 - y}{2} \text{log}(1 - \frac{f(x_i, W) + 1}{2})}
\end{align*}

\noindent Clearly the loss is zero for correctly classified datapoints and non-zero for missclassified datapoints. Furthermore an output of $\text{tanh}(x), x \rightarrow -1$ and  $\text{tanh}(x), x \rightarrow 1$ results in a low (probability) value respectively high value. By construction the activation function to be chosen is $\text{tanh}(\cdot)$

\exercise

Let $z_i = y_i - w x_i$

\begin{align*}
\gradf{E(w)}{w} &= \sumf{i=1}{N}{\partialf{z_i}E(w) \cdot \partialf{w}z_i} \tag{5.1} \\
\partialf{z_i}E(w) &= \ifelse{y_i - w x_i}{$\vert z \vert < 1$}{1}{} \tag{5.2} \\
\partialf{w}z_i &= -x_i \tag{5.3}
\end{align*}

\noindent Plugging (5.2), (5.3) in (5.1) we have

\begin{align*}
\gradf{E(w)}{w} &= \ifelse{-\sumf{i=1}{N}{ (y_i - w x_i) x_i} + \lambda w}{$\vert z \vert < 1$}{-\sumf{i=1}{N}{x_i} + \lambda w}{}
\end{align*}

\exercise

\noindent We would stop training approximately at iteration 50, because while the training error keeps decreasing we have an increase in the validation error, which is a sign of overfitting. After we take the weights of the network at this iteration we test it one final time on the test set. Note that we do not get the optimal test error as we would if we would continue with the training until iteration 80.

\exercise

\begin{align*}
y &= \text{log}(\sumf{i=1}{N}{ e^{x_i} }) \\
&= \text{log}(\sumf{i=1}{N}{ \frac{e^a}{e^a} e^{x_i} }) \\
&= \text{log}(e^a \sumf{i=1}{N}{e^{-a} e^{x_i} }) \\
&= \text{log}(e^a) + \text{log}(\sumf{i=1}{N}{e^{-a} e^{x_i} }) \\
&= a + \text{log}(\sumf{i=1}{N}{e^{x_i - a} })
\end{align*}

\exercise

\begin{align*}
\frac{e^{x_i}}{\sumf{i=1}{N}{e^{x_i}}} &= \frac{e^{a}}{e^{a}} \frac{e^{x_i}}{\sumf{i=1}{N}{e^{x_i}}} \\
&= \frac{e^{x_i - a}}{\sumf{i=1}{N}{e^{x_i - a}}}
\end{align*}

\exercise

\noindent The binary cross-entropy is equivalent to

\begin{align*}
-[y \text{log}(\sigma(x)) + (1 - y) \text{log}(1 - \sigma(x)) ] &= -[y \text{log}(\frac{e^x}{1 + e^x}) + (1 - y) \text{log}(1 - \frac{e^x}{1 + e^x}) ] \\
&= -[ y \text{log}e^x -y \text{log}(1 + e^x) + (1 - y)( -\text{log}(1 + e^x ))] \\
&= -[yx - \text{log}(1 + e^x )] \\
&= -yx + \text{log}(1 + e^x ) \\
&= -yx + \text{log}(e^x(e^{-x} + 1 )) \\
&= x -yx + \text{log}( 1 + e^{-x} ) \\
&= \text{max}(x,0) -yx + \text{log}( 1 + e^{-|x|} ) \tag{see below}
\end{align*}

\noindent The binary cross-entropy and in general any well-defined loss function should satisfy the following properties : For correctly classified datapoints the loss is zero and for incorrectly classified points it is non-zero. This can be easily verified for the binary cross-entropy, e.g. if x is a large positive score and y = 0, i.e. x is missclassified then we get a non-zero contribution to the loss by x and on conversely if y = 1, the loss is zero. The stable binary cross-entropy should also satisfy this properties.
\\
\noindent For $x \rightarrow -\infty$ we get an overflow  when implementing it on a machine. To overcome this issue, we can map any negative x to a positive value using the $\text{abs}(\cdot)$ function. To ensure that missclassified datapoints get penalized with a non-zero loss we have to add the term $\text{max}(x, 0)$ (In case of correctly classified datapoints this term either is zero or cancels out with $yx$) to the loss function.

\end{document}
