\documentclass[11pt]{article}

% ------------------------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% ------------------------------------------------------------------------------

\usepackage[margin=.8in,top=1.1in,bottom=1.1in]{geometry} % page layout
\usepackage{amsmath,amsthm,amssymb,amsfonts} % math things
\usepackage{graphicx} % include graphics
\usepackage{fancyhdr} % header customization
\usepackage{titlesec} % help with section naming
\usepackage{listings}% display source code
\usepackage{color}

%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},   commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}

%"mystyle" code listing set
\lstset{style=mystyle}

% naming sections
\titleformat{\section}{\bf}{Problem \thesection}{0.5em}{}
\newcommand{\exercise}{\section{}}

% headers
\pagestyle{fancy}
\fancyhf{} % clear all
\fancyhead[L]{ \sffamily\small Machine Learning I --- Homework}
\fancyhead[R]{\sffamily\small Page \thepage}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.2pt}
\markright{\hrulefill\quad}

\newcommand{\hwhead}[4]{
\begin{center}
\sffamily\large\bfseries Machine Learning I Worksheet #2
\vspace{2mm}
\normalfont

#2\\
#3\\
\texttt{#4}
\end{center}
\vspace{6mm} \hrule \vspace{4mm}
}

%%
\DeclareMathOperator{\E}{\mathbb{E}}
\newcommand{\intf}[4]{\int_{#1}^{#2} \! #3 \, \mathrm{d}#4}
\newcommand{\sumf}[3]{\sum_{#1}^{#2} #3}
\newcommand{\tf}[1]{{#1}^{\intercal}}
\newcommand{\ifelse}[4]{%
\begingroup
\begin{cases}
  {#1} \text{, if {#2}} \\
  {#3} \text{, otherwise {#4}}
\end{cases}
\endgroup
}

\newcommand{\gradf}[2]{\frac{\nabla_{#1}}{{\partial #2}}}
\newcommand{\partialf}[1]{\frac{\partial}{{\partial #1}}}

\begin{document}

\exercise

A neural network uses a hierarchical feature representation of the input space and adaptive basis functions to linearly separate the input space. The basis functions are adaptive in the sense that their coefficients are not hand-crafted but determined by learning.

\exercise

\begin{align*}
\text{tanh}(x) &= \frac{e^x - e^{-x}}{e^x + e^{-x}} \\
&= \frac{e^x}{e^x + e^{-x}} - \frac{e^{-x}}{e^x + e^{-x}} \\
&= \frac{1}{1 + e^{-2x}} - \frac{1}{1 + e^{2x}} \\
&= \sigma(2x) - \sigma(-2x) \\
&= \sigma(2x) - (1 - \sigma(2x)) \\
&= \sigma(2x) - 1 + \sigma(2x) \\
&= 2\sigma(2x) - 1
\end{align*}

It immediately follows that

\begin{align*}
\sigma(x) &= \frac{\text{tanh}(\frac{x}{2}) + 1}{2} \tag{2.1}
\end{align*}

\noindent By (2.1), if we take all the \textit{input} hidden-layer weights $\sigma$ multiply them by $\frac{1}{2}$ and scale the all \textit{output} hidden-layer weights by $\frac{1}{2}$ and add $\frac{1}{2}$ to their bias we get the exact same neural network function output using $\text{tanh}$ activation functions for the hidden layer.

\exercise

\begin{align*}
\partialf{x}(\text{tanh}(x)) &= \partialf{x}(\frac{e^x - e^{-x}}{e^x + e^{-x}}) \\
&= \frac{ ( e^x + e^{-x} )( e^x + e^{-x}) - ( e^x - e^{-x} )( e^x - e^{-x} )}{ ( e^x + e^{-x} )^2 } \\
&= 1 - \text{tanh}^2(x)
\end{align*}

\noindent It simplifies the mathematical derivations and the implementation, specifically the computation of the local gradient at an intermediate node and the backpropagation of the gradient to the lower layers.

\exercise

For deriving the new loss function, we follow a similar approach as in cf. Lecture Linear Classification (slide 28). We use $ \text{tanh}( \cdot ) $ instead of $\sigma( \cdot )$. To interpret $\text{Ber} \sim p( y | x)$ as probability we rescale it to be in $[0, 1]$.

\begin{align*}
p( y = 1 | x) &=\frac{\text{tanh}(x) + 1}{2}, \quad y = 1 \\
p( y = -1 | x) &=1 - \frac{\text{tanh}(x) + 1}{2}, \quad y = -1
\end{align*}
or more compactly as:
$x^{\frac{1 + y}{2}} \cdot (1 - x)^{\frac{1 - y}{2}}$

\begin{align*}
E(W) &= -\sumf{i = 1}{N}{ \frac{1 + y}{2} \text{log}(\frac{\text{tanh}(x_i, W) + 1}{2}) + \frac{1 - y}{2} \text{log}(1 - \frac{\text{tanh}(x_i, W) + 1}{2})} \\
&= -\sumf{i = 1}{N}{ \frac{1 + y}{2} \text{log}(\frac{f(x_i, W) + 1}{2}) + \frac{1 - y}{2} \text{log}(1 - \frac{f(x_i, W) + 1}{2})}
\end{align*}

\noindent Clearly the loss is zero for correctly classified datapoints and non-negative for missclassified datapoints. Furthermore an output of $\text{tanh}(x), x \rightarrow -1$ and  $\text{tanh}(x), x \rightarrow 1$ results in low respectively high probability value. 
\\
The activation function to be chosen is $\text{tanh}(\cdot)$

\exercise

Let $z_i = y_i - w x_i$

\begin{align*}
\gradf{E(w)}{w} &= \sumf{i=1}{N}{\partialf{z_i}E(w) \cdot \partialf{w}z_i} \tag{5.1} \\
\partialf{z_i}E(w) &= \ifelse{y_i - w x_i}{$\vert z \vert < 1$}{1}{} \tag{5.2} \\
\partialf{w}z_i &= -x_i \tag{5.3}
\end{align*}

\noindent Plugging (5.2), (5.3) in (5.1) we have

\begin{align*}
\gradf{E(w)}{w} &= \ifelse{-\sumf{i=1}{N}{ (y_i - w x_i) x_i} + \lambda w}{$\vert z \vert < 1$}{-\sumf{i=1}{N}{x_i} + \lambda w}{}
\end{align*}

\exercise

\noindent We would stop training approximately at iteration 50, because while the training error keeps decreasing we have an increase in the validation error, which is a sign of overfitting. After we take the weights of the network at this iteration we test it one final time on the test set. Note that we do not get the optimal test error as we would if we would continue with the training until iteration 80.

\exercise

\begin{align*}
y &= \text{log}(\sumf{i=1}{N}{ e^{x_i} }) \\
&= \text{log}(\sumf{i=1}{N}{ \frac{e^a}{e^a} e^{x_i} }) \\
&= \text{log}(e^a \sumf{i=1}{N}{e^{-a} e^{x_i} }) \\
&= \text{log}(e^a) + \text{log}(\sumf{i=1}{N}{e^{-a} e^{x_i} }) \\
&= a + \text{log}(\sumf{i=1}{N}{e^{x_i - a} })
\end{align*}

\exercise

\begin{align*}
\frac{e^{x_i}}{\sumf{i=1}{N}{e^{x_i}}} &= \frac{e^{a}}{e^{a}} \frac{e^{x_i}}{\sumf{i=1}{N}{e^{x_i}}} \\
&= \frac{e^{x_i - a}}{\sumf{i=1}{N}{e^{x_i - a}}}
\end{align*}

\exercise

\noindent The binary cross-entropy is equivalent to

\begin{align*}
-[y \text{log}(\sigma(x)) + (1 - y) \text{log}(1 - \sigma(x)) ] &= -[y \text{log}(\frac{e^x}{1 + e^x}) + (1 - y) \text{log}(1 - \frac{e^x}{1 + e^x}) ] \\
&= -[ y \text{log}e^x -y \text{log}(1 + e^x) + (1 - y)( -\text{log}(1 + e^x ))] \\
&= -[yx - \text{log}(1 + e^x )] \\
&= -yx + \text{log}(1 + e^x ) \tag{9.1} \\
&= -yx + \text{log}(e^x(e^{-x} + 1 )) \\
&= \text{log}(e^x) -yx +  \text{log}(e^{-x} + 1 ) \\
&= x -yx + \text{log}( 1 + e^{-x} ) \tag{9.2} \\
&= \text{max}(x,0) -yx + \text{log}( 1 + e^{-|x|} ) \tag{9.3}
\end{align*}

\noindent If $x > 0$, then (9.3) follows immediately by (9.2) else if $x < 0$ then plugging it in (9.1) we also immediately get (9.3).

\end{document}
