    \documentclass[11pt]{article}

% ------------------------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% ------------------------------------------------------------------------------

\usepackage[margin=.8in,top=1.1in,bottom=1.1in]{geometry} % page layout
\usepackage{amsmath,amsthm,amssymb,amsfonts} % math things
\usepackage{graphicx} % include graphics
\usepackage{fancyhdr} % header customization
\usepackage{titlesec} % help with section naming
\usepackage{listings}% display source code
\usepackage{color}

%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},   commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}

%"mystyle" code listing set
\lstset{style=mystyle}

% naming sections
\titleformat{\section}{\bf}{Problem \thesection}{0.5em}{}
\newcommand{\exercise}{\section{}}

% headers
\pagestyle{fancy}
\fancyhf{} % clear all
\fancyhead[L]{\sffamily\small Machine Learning I --- Homework}
\fancyhead[R]{\sffamily\small Page \thepage}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.2pt}
\markright{\hrulefill\quad}

\newcommand{\hwhead}[4]{
\begin{center}
\sffamily\large\bfseries Machine Learning I Worksheet #2
\vspace{2mm}
\normalfont

#2\\
#3\\
\texttt{#4}
\end{center}
\vspace{6mm} \hrule \vspace{4mm}
}

%%
\DeclareMathOperator{\E}{\mathbb{E}}
\newcommand{\intf}[4]{\int_{#1}^{#2} \! #3 \, \mathrm{d}#4}
\newcommand{\sumf}[3]{\sum_{#1}^{#2} #3}
\newcommand{\tf}[1]{{#1}^{\intercal}}
\newcommand{\ifelse}[4]{%
\begingroup
\begin{cases}
  {#1} \text{, if {#2}} \\
  {#3} \text{, if {#4}}
\end{cases}
\endgroup
}

\newcommand{\gradf}[1]{\frac{\nabla}{{\partial #1}}}
\newcommand{\partialf}[1]{\frac{\partial}{{\partial #1}}}

\begin{document}

\exercise

To enforce the necessary constraints, we introduce the Lagrangian multipliers $\lambda_{1}, ..., \lambda_{M}, \lambda_{M+1}$ and make an unconstrained maximization of 

$$L = \tf{\mathbf{u}_{M+1}} \mathbf{S} \mathbf{u}_{M+1}  + \lambda_{M+1}(1 - \tf{\mathbf{u}_{M+1}} \mathbf{u}_{M+1}) + \sumf{i=1}{M}{\lambda_i (\tf{\mathbf{u}_{M+1}} \mathbf{u}_{i} )}$$

\noindent Now we take the derivative of L with respect to a vector $\mathbf{u}_{M+1}$ and set it to zero
\begin{align*}
\partialf{\mathbf{u}_{M+1}} L = 0 &\Leftrightarrow 2 \mathbf{S} \mathbf{u}_{M+1} = 2 \lambda_{M+1} \mathbf{u}_{M+1} - \sumf{i=1}{M}{\lambda_i \mathbf{u}_{i}} \\
&\Leftrightarrow 2 \tf{\mathbf{u}_{M+1}} \mathbf{S} \mathbf{u}_{M+1} = 2 \lambda_{M+1} \tf{\mathbf{u}_{M+1}} \mathbf{u}_{M+1} - \sumf{i=1}{M}{\lambda_i \tf{\mathbf{u}_{M+1}} \mathbf{u}_{i}} \tag{left-multiply by $\tf{\mathbf{u}_{M+1}}$} \\
&\Leftrightarrow \tf{\mathbf{u}_{M+1}} \mathbf{S} \mathbf{u}_{M+1} = \lambda_{M+1} \tag{by orthonormality of $u_i$ and $\mathbf{u}_{M+1}$} \\
\end{align*}

\noindent  By induction step, we see that the variance in direction $\mathbf{u}_{M+1}$  is maximum when we set $\mathbf{u}_{M+1}$ equal to the eigenvector having the $(m+1)^{\text{th}}$ largest eigenvalue $\lambda_{M+1}$.

\exercise

\begin{align*}
\mathbf{x_i} &\sim \mathcal{N}(\mathbf{\mu_x}, \mathbf{\Phi_x}) \tag{$\mathbf{\mu_x} = \mathbf{\mu}$, $\mathbf{\Phi_x} = \mathbf{W} \tf{\mathbf{W}} + \Phi^2 \mathbf{I}$}
\end{align*}

\noindent From lecture we know that $\mathbf{y_i} = \mathbf{A} \mathbf{x_i} \sim \mathcal{N}(\mathbf{\mu_y}, \mathbf{\Phi_y}) $. Let's first derive the two moments

\begin{align*}
\mathbf{\mu_y} &= \mathbf{E}[y] \\
&= \mathbf{E}[\mathbf{A} \mathbf{x}] \\
&= \mathbf{A} \mathbf{E}[\mathbf{x}] \\
&= \mathbf{A} \mathbf{\mu_x}
\end{align*}

\begin{align*}
\mathbf{\Phi_y} &= \mathbf{E}[ (\mathbf{y} - \mathbf{\mu_y}) \tf{(\mathbf{y} - \mathbf{\mu_y})} ] \\
&= \mathbf{E}[ ( \mathbf{A} \mathbf{x} - \mathbf{A} \mathbf{\mu_x} ) \tf{( \mathbf{A} \mathbf{x} - \mathbf{A} \mathbf{\mu_x} ) } ] \\
&= \mathbf{E}[ ( \mathbf{A} (\mathbf{x} - \mathbf{\mu_x}) ) \tf{( \mathbf{A} (\mathbf{x} - \mathbf{\mu_x}) ) } ] \\
&= \mathbf{A} \mathbf{E}[ ( \mathbf{x} - \mathbf{\mu_x} ) \tf{( \mathbf{x} - \mathbf{\mu_x} ) } ] \mathbf{A} \\
&= \mathbf{A} \mathbf{\Phi_x} \mathbf{A} \\
\end{align*}

\noindent By pattern matching we have for the transformed Maximum Likelihood estimates

\begin{align*}
\mathbf{\mu}_{y_{ML}} &= \mathbf{A} \mathbf{\mu}_{ML} \\
\mathbf{\Phi}_{y_{ML}} &= \mathbf{A} \mathbf{\Phi}_{ML} \tf{\mathbf{A}} \\
\mathbf{W}_{y_{ML}} &= \mathbf{A} \mathbf{W}_{ML}
\end{align*}

\noindent By orthogonality $ \mathbf{A} \tf{\mathbf{A}} = \tf{\mathbf{A}} \mathbf{A} = \mathbf{I}$ and $ \mathbf{\Phi} = \sigma^2  \mathbf{I}$ we have
\begin{align*}
\mathbf{A} \mathbf{\Phi} \tf{\mathbf{A}} &= \mathbf{A} \sigma^2  \mathbf{I} \tf{\mathbf{A}} \\
&= \sigma^2 \mathbf{A} \tf{\mathbf{A}} \\
&= \sigma^2 \mathbf{I}
\end{align*}

\exercise

Let $\mathbf{x} \in \mathbb{R}^5$ hold the movie ratings given by Leslie.
\\
\noindent By the SVD projection in concept space we have
$$\mathbf{V}  \cdot \mathbf{x} = \tf{[1.74, 2.84]}$$
\\
\noindent By SVD decomposition and reconstruction of the input using the projected space we have
$$ \tf{[1.74, 2.84]} \cdot \tf{\mathbf{V}} = \tf{[1.0092, 1.0092, 1.0092, 2.0164, 2.0164]} $$

\noindent E.g. we can predict that Leslie will rate Titanic movie with 2.0164.

\exercise

See below.

\end{document}
